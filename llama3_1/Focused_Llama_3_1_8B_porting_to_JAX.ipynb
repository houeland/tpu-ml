{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8Rvu7VN7zyhG",
        "aVO3q402UVvi",
        "Cry-dl1iz8FV",
        "fb6jQguT_DOP",
        "ItTS0n19--Bk",
        "oDlKAxaMsNjz",
        "46EJonhZwrTY",
        "QbxhYibnHdOh",
        "RzR6Hp6FHWZ3",
        "y-0eKR5VGWyp",
        "m_198F1iGKoE",
        "m2E8XJpJmqc_",
        "v6QeTa1sZaqQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "gcloud compute tpus tpu-vm create jax-tpu-v4-8 --zone us-central2-b --project tpu-research-cloud-project --accelerator-type v4-8 --version tpu-vm-v4-base\n",
        "\n",
        "sudo apt-get install python3.9 libpython3.9 --assume-yes\n",
        "virtualenv --python='/usr/bin/python3.9' virtualenv-jax\n",
        "source virtualenv-jax/bin/activate\n",
        "\n",
        "pip install 'jax[tpu]' optax flax ml-collections notebook -f https://storage.googleapis.com/jax-releases/libtpu_releases.html -f https://storage.googleapis.com/jax-releases/jax_releases.html torch pydantic pydantic_core tiktoken json-strong-typing fairscale blobfile\n",
        "\n",
        "\n",
        "\n",
        "gcloud compute tpus tpu-vm ssh jax-tpu-v4-8 --project=tpu-research-cloud-project --zone=us-central2-b -- -L 8866:localhost:8866\n",
        "\n",
        "screen -U\n",
        "\n",
        "sudo mkdir /mnt/ramdisk\n",
        "sudo mount -t tmpfs -o size=25G tmpfs /mnt/ramdisk\n",
        "mkdir /mnt/ramdisk/llama3_1\n",
        "time gsutil -m cp -r gs://trc-ml-us-central2/llama/Meta-Llama-3.1-8B-Instruct_split/ /mnt/ramdisk/llama3_1/\n",
        "\n",
        "source virtualenv-jax/bin/activate\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8866 --NotebookApp.port_retries=0 --no-browser\n",
        "```"
      ],
      "metadata": {
        "id": "6Vub2YaowMRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "lNFdkZDLm76D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG9QO89iWxRP",
        "outputId": "e07d2bcb-096f-4b22-8cd0-a7def4f675dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4.30\n",
            "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "print(jax.__version__)\n",
        "print(jax.devices())\n",
        "\n",
        "# from jax import config\n",
        "jax.config.update(\"jax_numpy_rank_promotion\", \"raise\")\n",
        "jax.config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D72d-ZubxQLt",
        "outputId": "b17c567a-0d10-4a43-d8ef-6f4a9b36a268"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch/JAX interop + generic utils"
      ],
      "metadata": {
        "id": "8Rvu7VN7zyhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import pathlib\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=20, floatmode='fixed')\n",
        "\n",
        "def deets(t):\n",
        "  return t.shape, t.dtype, t.device, torch.sum(torch.abs(t.flatten().to(dtype=torch.float64))).detach().numpy()\n",
        "\n",
        "def deet(t):\n",
        "  return t.shape, t.dtype, t.devices(), np.array(jnp.sum(jnp.abs(t.reshape(-1).astype(jnp.float64))))\n",
        "\n",
        "def deets(t):\n",
        "  x = t.to(dtype=torch.float64).detach().numpy()\n",
        "  return t.shape, t.dtype, t.device, np.sum(np.abs(x.flatten()))\n",
        "\n",
        "def deet(t):\n",
        "  x = np.array(t.astype(jnp.float64).T)\n",
        "  return t.shape, t.dtype, t.devices(), np.sum(np.abs(x.flatten()))\n",
        "\n",
        "def deetnosum(t):\n",
        "  return t.shape, t.dtype, t.devices()\n",
        "\n",
        "def deetnodev(t):\n",
        "  return t.shape, t.dtype\n",
        "\n",
        "print(deets(torch.ones(4,2,3)))\n",
        "\n",
        "print(deet(jnp.ones((4,2,3))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stgv0gXicnGq",
        "outputId": "fd80188d-e5f9-4550-8482-93b1aef79d1d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(torch.Size([4, 2, 3]), torch.float32, device(type='cpu'), np.float64(24.0))\n",
            "((4, 2, 3), dtype('float64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(24.0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.dlpack as jdp\n",
        "import torch.utils.dlpack as tdp\n",
        "\n",
        "def torch_to_jax(x, device):\n",
        "  return jdp.from_dlpack(tdp.to_dlpack(x), device=device)\n",
        "\n",
        "def jax_to_torch(x):\n",
        "  return tdp.from_dlpack(jdp.to_dlpack(x))\n",
        "\n",
        "def jax_cpu():\n",
        "  print(\"JAX_CPU()!!!\")\n",
        "  return jax.devices(\"cpu\")[0]\n",
        "\n",
        "def jax_to_cpu(x):\n",
        "  print(\"jax_to_cpu()!!!\")\n",
        "  return jax.device_put(x, jax_cpu())\n",
        "\n",
        "def jax_tpu():\n",
        "  return jax.devices(\"tpu\")[0]\n",
        "\n",
        "def f():\n",
        "  t = torch.rand((1,3))\n",
        "  print(\"t\", deets(t), t)\n",
        "  tj = torch_to_jax(t, jax_cpu())\n",
        "  print(\"t=>j\", deet(tj), tj)\n",
        "  rng = jax.random.key(42)\n",
        "  j = jax_to_cpu(jax.random.uniform(rng, (3,1)))\n",
        "  print(\"j\", deet(j), j)\n",
        "  jt = jax_to_torch(j)\n",
        "  print(\"j=>t\", deets(jt), jt)\n",
        "\n",
        "f()\n",
        "\n",
        "def f():\n",
        "  torch_tensor = torch.randn(2, 3, dtype=torch.bfloat16)\n",
        "  print(\"torch_tensor\", torch_tensor)\n",
        "  torch_np = torch_tensor.to(dtype=torch.float64).numpy()\n",
        "  print(\"torch numpy conversion\", torch_np, torch_np.dtype, type(torch_np))\n",
        "\n",
        "  jax_array = torch_to_jax(torch_tensor, jax_cpu())\n",
        "  print(\"jax_array\", jax_array, jax_array.dtype)\n",
        "  jax_np = np.array(jnp.astype(jax_array, jnp.float64))\n",
        "  print(\"jax numpy conversion\", jax_np, jax_np.dtype, type(jax_np))\n",
        "\n",
        "  print(\"diff\", np.sum(np.abs(jax_np - torch_np)), jax_np - torch_np)\n",
        "\n",
        "f()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7EFTN0LbYeB",
        "outputId": "755c094c-bc5b-4668-e88c-26f90cd9ec78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t (torch.Size([1, 3]), torch.float32, device(type='cpu'), np.float64(2.0716968178749084)) tensor([[0.2830, 0.9367, 0.8520]])\n",
            "JAX_CPU()!!!\n",
            "t=>j ((1, 3), dtype('float32'), {CpuDevice(id=0)}, np.float64(2.0716968178749084)) [[0.28296488523483276367 0.93671125173568725586 0.85202068090438842773]]\n",
            "jax_to_cpu()!!!\n",
            "JAX_CPU()!!!\n",
            "j ((3, 1), dtype('float64'), {CpuDevice(id=0)}, np.float64(2.4713129808532663)) [[0.72981889690463219722]\n",
            " [0.86919382428439107002]\n",
            " [0.87230025966424307171]]\n",
            "j=>t (torch.Size([3, 1]), torch.float64, device(type='cpu'), np.float64(2.4713129808532663)) tensor([[0.7298],\n",
            "        [0.8692],\n",
            "        [0.8723]], dtype=torch.float64)\n",
            "torch_tensor tensor([[-0.8867,  0.0630, -1.4844],\n",
            "        [ 1.7812,  2.1719,  0.1934]], dtype=torch.bfloat16)\n",
            "torch numpy conversion [[-0.88671875000000000000  0.06298828125000000000 -1.48437500000000000000]\n",
            " [ 1.78125000000000000000  2.17187500000000000000  0.19335937500000000000]] float64 <class 'numpy.ndarray'>\n",
            "JAX_CPU()!!!\n",
            "jax_array [[-0.886719 0.0629883 -1.48438]\n",
            " [1.78125 2.17188 0.193359]] bfloat16\n",
            "jax numpy conversion [[-0.88671875000000000000  0.06298828125000000000 -1.48437500000000000000]\n",
            " [ 1.78125000000000000000  2.17187500000000000000  0.19335937500000000000]] float64 <class 'numpy.ndarray'>\n",
            "diff 0.0 [[0.00000000000000000000 0.00000000000000000000 0.00000000000000000000]\n",
            " [0.00000000000000000000 0.00000000000000000000 0.00000000000000000000]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint_enabled = True\n",
        "\n",
        "def enable_pprint(v):\n",
        "  global pprint_enabled\n",
        "  pprint_enabled = v\n",
        "\n",
        "def pprint(*args):\n",
        "  if pprint_enabled:\n",
        "    print(*args)\n",
        "\n",
        "def pprint_d(msg, x):\n",
        "  if pprint_enabled:\n",
        "    pprint(msg, deet(x))\n"
      ],
      "metadata": {
        "id": "t3i7iXWuICiz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_torch_weights(filename, device=None):\n",
        "  torch_weights = torch.load(\n",
        "      filename, weights_only=True, map_location=torch.device(\"cpu\"), mmap=True\n",
        "  )\n",
        "  # print(\"torch_weights\", deets(torch_weights))\n",
        "  jax_weights = torch_to_jax(torch_weights, device=device)\n",
        "  # print(\"jax_weights\", deet(jax_weights))\n",
        "  return jax_weights\n"
      ],
      "metadata": {
        "id": "Wlii7CfvIVEt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "\n",
        "NUM_DEVICES = 4\n",
        "\n",
        "jax_mesh_x = jax.sharding.Mesh(devices=mesh_utils.create_device_mesh([NUM_DEVICES]), axis_names=('x'))\n",
        "jax_sharding_x = jax.sharding.NamedSharding(jax_mesh_x, P('x'))\n",
        "\n",
        "def shard_array(arr):\n",
        "  shape = arr.shape\n",
        "  # print(\"arr\", deet(arr))\n",
        "  split = jnp.split(arr, NUM_DEVICES, axis=0)\n",
        "  tpu = jax.devices(\"tpu\")\n",
        "  for i in range(NUM_DEVICES):\n",
        "    split[i] = jax.device_put(split[i], tpu[i])\n",
        "  # print(\"split\", [deet(x) for x in split])\n",
        "  recombined = jax.make_array_from_single_device_arrays(shape, jax_sharding_x, split)\n",
        "  return recombined\n",
        "\n",
        "def load_torch_weights_sharded(filename, device=None):\n",
        "  torch_weights = torch.load(\n",
        "      filename, weights_only=True, map_location=torch.device(\"cpu\"), mmap=True\n",
        "  )\n",
        "  # print(\"torch_weights\", deets(torch_weights))\n",
        "  cpu_device = jax.devices(\"cpu\")[0]\n",
        "  jax_weights = torch_to_jax(torch_weights, device=cpu_device)\n",
        "  # jax_weights = jax.device_put(x, jax.sharding.NamedSharding(jax_mesh_2_2, P('x', 'y')))\n",
        "  # print(\"jax_weights\", deet(jax_weights))\n",
        "  jax_weights = shard_array(jax_weights)\n",
        "  return jax_weights\n",
        "\n",
        "  # mesh = Mesh(devices=mesh_utils.create_device_mesh([4]), axis_names=('x'))\n",
        "  # print(\"mesh\", mesh)\n",
        "  # print(\"sharding\", sharding)\n",
        "  # arr = jax.make_array_from_single_device_arrays((4*1024,1024,1024), sharding, [y1,y2,y3,y4])\n",
        "\n",
        "def f():\n",
        "  with jax.default_device(jax.devices(\"cpu\")[0]):\n",
        "    nums = jnp.arange(8*50*10)\n",
        "    arr = nums.reshape((8,50,10))\n",
        "  print(\"arr\", deet(arr))\n",
        "  for x in range(8):\n",
        "    print(x, deet(arr[x]))\n",
        "  arr = shard_array(arr)\n",
        "  print(\"arr\", deet(arr))\n",
        "  for x in range(8):\n",
        "    print(x, deet(arr[x]))\n",
        "\n",
        "f()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIGzkqYnf9wo",
        "outputId": "b407481a-9374-4345-a755-bc387a92c1eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arr ((8, 50, 10), dtype('int64'), {CpuDevice(id=0)}, np.float64(7998000.0))\n",
            "0 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(124750.0))\n",
            "1 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(374750.0))\n",
            "2 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(624750.0))\n",
            "3 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(874750.0))\n",
            "4 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(1124750.0))\n",
            "5 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(1374750.0))\n",
            "6 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(1624750.0))\n",
            "7 ((50, 10), dtype('int64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(1874750.0))\n",
            "arr ((8, 50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(7998000.0))\n",
            "0 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(124750.0))\n",
            "1 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(374750.0))\n",
            "2 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(1124750.0))\n",
            "3 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(1374750.0))\n",
            "4 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(624750.0))\n",
            "5 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(874750.0))\n",
            "6 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(1624750.0))\n",
            "7 ((50, 10), dtype('int64'), {TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}, np.float64(1874750.0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory profiling"
      ],
      "metadata": {
        "id": "aVO3q402UVvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f():\n",
        "  arr = jnp.zeros((int(512*2),1024,1024))\n",
        "  print(arr.shape, arr.devices())\n",
        "\n",
        "f()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljgLtgUEZ1nV",
        "outputId": "322b1de4-ca56-4385-dda8-cbad9364a872"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1024, 1024, 1024) {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax_transformer_params_tpu = None\n",
        "jax_freqs_cis = None"
      ],
      "metadata": {
        "id": "ONk9pG9Si9UH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.clear_caches()"
      ],
      "metadata": {
        "id": "-qXHH4N4lRuH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %reset -f"
      ],
      "metadata": {
        "id": "_JOu6n7aoNdS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "def f():\n",
        "  tagged = {}\n",
        "\n",
        "  def tagit(x, label):\n",
        "      tagged[id(x)] = label\n",
        "  if jax_transformer_params_tpu is not None:\n",
        "    jax.tree_util.tree_map(lambda x: tagit(x, \"params\"), jax_transformer_params_tpu)\n",
        "  if jax_freqs_cis is not None:\n",
        "    tagged[id(jax_freqs_cis)] = \"freqs_cis\"\n",
        "\n",
        "  arrs = jax.live_arrays(\"tpu\")\n",
        "  cnt = collections.Counter()\n",
        "  tot = collections.Counter()\n",
        "  for idx, arr in enumerate(arrs):\n",
        "    sz = jnp.prod(jnp.array(arr.shape))*2\n",
        "    tag = tagged.get(id(arr))\n",
        "    # if tag is None or True:\n",
        "      # print(idx, deetnosum(arr), sz // 1_000_000, tag)\n",
        "    cnt[tag] += 1\n",
        "    tot[tag] += sz / 1_000_000\n",
        "  # print(cnt, tot // 1_000_000)\n",
        "  for k in cnt:\n",
        "    print(k, cnt[k], \"%.0fM\" % (tot[k] // 1))\n",
        "\n",
        "f()\n",
        "\n",
        "dump_mem = f\n",
        "\n",
        "# -- init --\n",
        "# freqs_cis 1 0M\n",
        "# params 291 16060M\n",
        "\n",
        "# -- hmm --\n",
        "# None 194 6442M\n",
        "# freqs_cis 1 0M\n",
        "# params 291 16060M"
      ],
      "metadata": {
        "id": "zvntOr-VUX43"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Cry-dl1iz8FV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### llama_models datatypes"
      ],
      "metadata": {
        "id": "fb6jQguT_DOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the terms described in the LICENSE file in\n",
        "# top-level folder for each specific model found within the models/ directory at\n",
        "# the top-level of this source tree.\n",
        "\n",
        "from enum import Enum\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from strong_typing.schema import json_schema_type\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class SamplingStrategy(Enum):\n",
        "    greedy = \"greedy\"\n",
        "    top_p = \"top_p\"\n",
        "    top_k = \"top_k\"\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class SamplingParams(BaseModel):\n",
        "    strategy: SamplingStrategy = SamplingStrategy.greedy\n",
        "\n",
        "    temperature: Optional[float] = 0.0\n",
        "    top_p: Optional[float] = 0.95\n",
        "    top_k: Optional[int] = 0\n",
        "    max_tokens: Optional[int] = 0\n",
        "    repetition_penalty: Optional[float] = 1.0\n",
        "\n",
        "\n",
        "@json_schema_type(\n",
        "    schema={\n",
        "        \"description\": \"The format in which weights are specified. This does not necessarily always equal what quantization is desired at runtime since there can be on-the-fly conversions done.\",\n",
        "    }\n",
        ")\n",
        "class CheckpointQuantizationFormat(Enum):\n",
        "    # default format\n",
        "    bf16 = \"bf16\"\n",
        "\n",
        "    # used for enabling fp8_rowwise inference, some weights are bf16\n",
        "    fp8_mixed = \"fp8_mixed\"\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class ModelSKU(Enum):\n",
        "    llama3_1_8b = \"llama3_1_8b\"\n",
        "    llama3_1_70b = \"llama3_1_70b\"\n",
        "    llama3_1_405b_fp8_mp8 = \"llama3_1_405b_fp8_mp8\"\n",
        "    llama3_1_405b_bf16_mp8 = \"llama3_1_405b_bf16_mp8\"\n",
        "    llama3_1_405b_bf16_mp16 = \"llama3_1_405b_bf16_mp16\"\n",
        "\n",
        "    llama3_1_8b_instruct = \"llama3_1_8b_instruct\"\n",
        "    llama3_1_70b_instruct = \"llama3_1_70b_instruct\"\n",
        "    llama3_1_405b_instruct_fp8_mp8 = \"llama3_1_405b_instruct_fp8_mp8\"\n",
        "    llama3_1_405b_instruct_bf16_mp8 = \"llama3_1_405b_instruct_bf16_mp8\"\n",
        "    llama3_1_405b_instruct_bf16_mp16 = \"llama3_1_405b_instruct_bf16_mp16\"\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class HardwareRequirements(BaseModel):\n",
        "    memory_gb_per_gpu: int\n",
        "    gpu_count: int\n",
        "\n",
        "\n",
        "@json_schema_type(\n",
        "    schema={\n",
        "        \"description\": \"The model family and SKU of the model along with other parameters corresponding to the model.\"\n",
        "    }\n",
        ")\n",
        "class ModelDefinition(BaseModel):\n",
        "    sku: ModelSKU\n",
        "    description_markdown: str\n",
        "    max_seq_length: int\n",
        "    huggingface_id: Optional[str] = None\n",
        "    hardware_requirements: HardwareRequirements\n",
        "    quantization_format: CheckpointQuantizationFormat = (\n",
        "        CheckpointQuantizationFormat.bf16\n",
        "    )\n",
        "    recommended_sampling_params: Optional[SamplingParams] = None\n",
        "    model_args: Dict[str, Any]\n",
        "\n",
        "\n",
        "# TODO: resolve these types against the model SKUs above\n",
        "@json_schema_type(\n",
        "    schema={\n",
        "        \"description\": \"The type of the model. This is used to determine the model family and SKU.\"\n",
        "    }\n",
        ")\n",
        "class PretrainedModel(Enum):\n",
        "    llama3_8b = \"llama3_8b\"\n",
        "    llama3_70b = \"llama3_70b\"\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class InstructModel(Enum):\n",
        "    llama3_8b_chat = \"llama3_8b_chat\"\n",
        "    llama3_70b_chat = \"llama3_70b_chat\"\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class RewardModel(Enum):\n",
        "    llama3_70b_reward = \"llama3_70b_reward\"\n",
        "    llama3_405b_reward = \"llama3_405b_reward\""
      ],
      "metadata": {
        "id": "2Flc7RG8_EsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "849f33e4-d848-4d6f-d0f7-271c94e4fa7a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/houeland/virtualenv-jax/lib/python3.9/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_args\" in ModelDefinition has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### datatypes"
      ],
      "metadata": {
        "id": "ItTS0n19--Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the terms described in the LICENSE file in\n",
        "# top-level folder for each specific model found within the models/ directory at\n",
        "# the top-level of this source tree.\n",
        "\n",
        "from enum import Enum\n",
        "from typing import Dict, List, Literal, Optional, Union\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from strong_typing.schema import json_schema_type\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class Role(Enum):\n",
        "    system = \"system\"\n",
        "    user = \"user\"\n",
        "    assistant = \"assistant\"\n",
        "    ipython = \"ipython\"\n",
        "\n",
        "\n",
        "@json_schema_type(\n",
        "    schema={\"type\": \"string\", \"format\": \"uri\", \"pattern\": \"^(https?://|file://|data:)\"}\n",
        ")\n",
        "class URL(BaseModel):\n",
        "    uri: str\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.uri\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class Attachment(BaseModel):\n",
        "    url: URL\n",
        "    mime_type: str\n",
        "\n",
        "\n",
        "InterleavedTextAttachment = Union[\n",
        "    str,\n",
        "    Attachment,\n",
        "    List[Union[str, Attachment]],\n",
        "]\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class BuiltinTool(Enum):\n",
        "    brave_search = \"brave_search\"\n",
        "    wolfram_alpha = \"wolfram_alpha\"\n",
        "    photogen = \"photogen\"\n",
        "    code_interpreter = \"code_interpreter\"\n",
        "\n",
        "\n",
        "Primitive = Union[str, int, float, bool, None]\n",
        "RecursiveType = Union[Primitive, List[Primitive], Dict[str, Primitive]]\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class ToolCall(BaseModel):\n",
        "    call_id: str\n",
        "    tool_name: Union[BuiltinTool, str]\n",
        "    arguments: Dict[str, RecursiveType]\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class ToolResponse(BaseModel):\n",
        "    call_id: str\n",
        "    tool_name: Union[BuiltinTool, str]\n",
        "    content: InterleavedTextAttachment\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class ToolParamDefinition(BaseModel):\n",
        "    param_type: str\n",
        "    description: Optional[str] = None\n",
        "    required: Optional[bool] = True\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class ToolDefinition(BaseModel):\n",
        "    tool_name: Union[BuiltinTool, str]\n",
        "    description: Optional[str] = None\n",
        "    parameters: Optional[Dict[str, ToolParamDefinition]] = None\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class UserMessage(BaseModel):\n",
        "    role: Literal[Role.user.value] = Role.user.value\n",
        "    content: InterleavedTextAttachment\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class SystemMessage(BaseModel):\n",
        "    role: Literal[Role.system.value] = Role.system.value\n",
        "    content: InterleavedTextAttachment\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class ToolResponseMessage(BaseModel):\n",
        "    role: Literal[Role.ipython.value] = Role.ipython.value\n",
        "    # it was nice to re-use the ToolResponse type, but having all messages\n",
        "    # have a `content` type makes things nicer too\n",
        "    call_id: str\n",
        "    tool_name: Union[BuiltinTool, str]\n",
        "    content: InterleavedTextAttachment\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class StopReason(Enum):\n",
        "    end_of_turn = \"end_of_turn\"\n",
        "    end_of_message = \"end_of_message\"\n",
        "    out_of_tokens = \"out_of_tokens\"\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class TokenLogProbs(BaseModel):\n",
        "    logprobs_by_token: Dict[str, float]\n",
        "\n",
        "\n",
        "@json_schema_type\n",
        "class CompletionMessage(BaseModel):\n",
        "    role: Literal[Role.assistant.value] = Role.assistant.value\n",
        "    content: InterleavedTextAttachment\n",
        "    stop_reason: StopReason\n",
        "    tool_calls: List[ToolCall] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "Message = Annotated[\n",
        "    Union[\n",
        "        UserMessage,\n",
        "        SystemMessage,\n",
        "        ToolResponseMessage,\n",
        "        CompletionMessage,\n",
        "    ],\n",
        "    Field(discriminator=\"role\"),\n",
        "]"
      ],
      "metadata": {
        "id": "m2XKxrtI_AIV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â tiktoken"
      ],
      "metadata": {
        "id": "oDlKAxaMsNjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the terms described in the LICENSE file in\n",
        "# top-level folder for each specific model found within the models/ directory at\n",
        "# the top-level of this source tree.\n",
        "\n",
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
        "\n",
        "import os\n",
        "from logging import getLogger\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    AbstractSet,\n",
        "    cast,\n",
        "    Collection,\n",
        "    Dict,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Literal,\n",
        "    Optional,\n",
        "    Sequence,\n",
        "    Union,\n",
        ")\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "\n",
        "\n",
        "# The tiktoken tokenizer can handle <=400k chars without\n",
        "# pyo3_runtime.PanicException.\n",
        "TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
        "\n",
        "# https://github.com/openai/tiktoken/issues/195\n",
        "# Here we iterate over subsequences and split if we exceed the limit\n",
        "# of max consecutive non-whitespace or whitespace characters.\n",
        "MAX_NO_WHITESPACES_CHARS = 25_000\n",
        "\n",
        "\n",
        "class Tokenizer:\n",
        "    \"\"\"\n",
        "    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    special_tokens: Dict[str, int]\n",
        "\n",
        "    num_reserved_special_tokens = 256\n",
        "\n",
        "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
        "\n",
        "    def __init__(self, model_path: str):\n",
        "        \"\"\"\n",
        "        Initializes the Tokenizer with a Tiktoken model.\n",
        "\n",
        "        Args:\n",
        "            model_path (str): The path to the Tiktoken model file.\n",
        "        \"\"\"\n",
        "        assert os.path.isfile(model_path), model_path\n",
        "\n",
        "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
        "        num_base_tokens = len(mergeable_ranks)\n",
        "        special_tokens = [\n",
        "            \"<|begin_of_text|>\",\n",
        "            \"<|end_of_text|>\",\n",
        "            \"<|reserved_special_token_0|>\",\n",
        "            \"<|reserved_special_token_1|>\",\n",
        "            \"<|finetune_right_pad_id|>\",\n",
        "            \"<|step_id|>\",\n",
        "            \"<|start_header_id|>\",\n",
        "            \"<|end_header_id|>\",\n",
        "            \"<|eom_id|>\",  # end of message\n",
        "            \"<|eot_id|>\",  # end of turn\n",
        "            \"<|python_tag|>\",\n",
        "        ]\n",
        "        reserved_tokens = [\n",
        "            f\"<|reserved_special_token_{2 + i}|>\"\n",
        "            for i in range(self.num_reserved_special_tokens - len(special_tokens))\n",
        "        ]\n",
        "        special_tokens = special_tokens + reserved_tokens\n",
        "\n",
        "        self.special_tokens = {\n",
        "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
        "        }\n",
        "        self.model = tiktoken.Encoding(\n",
        "            name=Path(model_path).name,\n",
        "            pat_str=self.pat_str,\n",
        "            mergeable_ranks=mergeable_ranks,\n",
        "            special_tokens=self.special_tokens,\n",
        "        )\n",
        "\n",
        "        self.n_words: int = num_base_tokens + len(special_tokens)\n",
        "        # BOS / EOS token IDs\n",
        "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
        "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
        "        self.eot_id: int = self.special_tokens[\"<|eot_id|>\"]\n",
        "        self.eom_id: int = self.special_tokens[\"<|eom_id|>\"]\n",
        "        self.python_tag_id = self.special_tokens[\"<|python_tag|>\"]\n",
        "        self.pad_id: int = self.special_tokens[\"<|finetune_right_pad_id|>\"]\n",
        "        self.stop_tokens = [\n",
        "            self.special_tokens[\"<|eom_id|>\"],\n",
        "            self.special_tokens[\"<|eot_id|>\"],\n",
        "        ]\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        s: str,\n",
        "        *,\n",
        "        bos: bool,\n",
        "        eos: bool,\n",
        "        allowed_special: Optional[Union[Literal[\"all\"], AbstractSet[str]]] = None,\n",
        "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Encodes a string into a list of token IDs.\n",
        "\n",
        "        Args:\n",
        "            s (str): The input string to be encoded.\n",
        "            bos (bool): Whether to prepend the beginning-of-sequence token.\n",
        "            eos (bool): Whether to append the end-of-sequence token.\n",
        "            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n",
        "            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n",
        "\n",
        "        Returns:\n",
        "            list[int]: A list of token IDs.\n",
        "\n",
        "        By default, setting disallowed_special=() encodes a string by ignoring\n",
        "        special tokens. Specifically:\n",
        "        - Setting `disallowed_special` to () will cause all text corresponding\n",
        "          to special tokens to be encoded as natural text (insteading of raising\n",
        "          an error).\n",
        "        - Setting `allowed_special` to \"all\" will treat all text corresponding\n",
        "          to special tokens to be encoded as special tokens.\n",
        "        \"\"\"\n",
        "        if allowed_special is None:\n",
        "            allowed_special = set()\n",
        "        assert type(s) is str\n",
        "\n",
        "        substrs = (\n",
        "            substr\n",
        "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
        "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
        "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
        "            )\n",
        "        )\n",
        "        t: List[int] = []\n",
        "        for substr in substrs:\n",
        "            t.extend(\n",
        "                self.model.encode(\n",
        "                    substr,\n",
        "                    allowed_special=allowed_special,\n",
        "                    disallowed_special=disallowed_special,\n",
        "                )\n",
        "            )\n",
        "        if bos:\n",
        "            t.insert(0, self.bos_id)\n",
        "        if eos:\n",
        "            t.append(self.eos_id)\n",
        "        return t\n",
        "\n",
        "    def decode(self, t: Sequence[int]) -> str:\n",
        "        \"\"\"\n",
        "        Decodes a list of token IDs into a string.\n",
        "\n",
        "        Args:\n",
        "            t (List[int]): The list of token IDs to be decoded.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n",
        "        return self.model.decode(cast(List[int], t))\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_whitespaces_or_nonwhitespaces(\n",
        "        s: str, max_consecutive_slice_len: int\n",
        "    ) -> Iterator[str]:\n",
        "        \"\"\"\n",
        "        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n",
        "        consecutive whitespaces or consecutive non-whitespaces.\n",
        "        \"\"\"\n",
        "        current_slice_len = 0\n",
        "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
        "        slice_start = 0\n",
        "\n",
        "        for i in range(len(s)):\n",
        "            is_now_space = s[i].isspace()\n",
        "\n",
        "            if current_slice_is_space ^ is_now_space:\n",
        "                current_slice_len = 1\n",
        "                current_slice_is_space = is_now_space\n",
        "            else:\n",
        "                current_slice_len += 1\n",
        "                if current_slice_len > max_consecutive_slice_len:\n",
        "                    yield s[slice_start:i]\n",
        "                    slice_start = i\n",
        "                    current_slice_len = 1\n",
        "        yield s[slice_start:]"
      ],
      "metadata": {
        "id": "NURY8psBlkju"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatFormat"
      ],
      "metadata": {
        "id": "46EJonhZwrTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the terms described in the LICENSE file in\n",
        "# top-level folder for each specific model found within the models/ directory at\n",
        "# the top-level of this source tree.\n",
        "\n",
        "import uuid\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "@dataclass\n",
        "class ModelInput:\n",
        "    tokens: List[int]\n",
        "\n",
        "\n",
        "class ChatFormat:\n",
        "    possible_headers: Dict[Role, str]\n",
        "\n",
        "    def __init__(self, tokenizer: Tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.possible_headers = {\n",
        "            role: f\"<|start_header_id|>{role.value}<|end_header_id|>\\n\\n\"\n",
        "            for role in Role\n",
        "        }\n",
        "\n",
        "    def encode_header(self, role: str) -> List[int]:\n",
        "        tokens = []\n",
        "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
        "        tokens.extend(self.tokenizer.encode(role, bos=False, eos=False))\n",
        "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
        "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
        "        return tokens\n",
        "\n",
        "    def encode_message(self, message: Message) -> List[int]:\n",
        "        tokens = self.encode_header(message.role)\n",
        "\n",
        "        def _process_content(content: InterleavedTextAttachment):\n",
        "            def _process(c):\n",
        "                if isinstance(c, str):\n",
        "                    tokens.extend(self.tokenizer.encode(c, bos=False, eos=False))\n",
        "\n",
        "            if isinstance(content, str):\n",
        "                _process(content)\n",
        "            elif isinstance(content, list):\n",
        "                for c in content:\n",
        "                    _process(c)\n",
        "\n",
        "        if isinstance(message, CompletionMessage) and len(message.tool_calls) > 0:\n",
        "            tokens.append(self.tokenizer.special_tokens[\"<|python_tag|>\"])\n",
        "\n",
        "        _process_content(message.content)\n",
        "\n",
        "        if isinstance(message, CompletionMessage):\n",
        "            for t in message.tool_calls:\n",
        "                content = ToolUtils.encode_tool_call(t)\n",
        "                _process_content(content)\n",
        "\n",
        "        eom = False\n",
        "        if isinstance(message, CompletionMessage):\n",
        "            eom = message.stop_reason == StopReason.end_of_message\n",
        "\n",
        "        tokens.append(\n",
        "            self.tokenizer.special_tokens[\"<|eom_id|>\" if eom else \"<|eot_id|>\"]\n",
        "        )\n",
        "        return tokens\n",
        "\n",
        "    def encode_dialog_prompt(self, messages: List[Message]) -> ModelInput:\n",
        "        tokens = []\n",
        "        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n",
        "        for message in messages:\n",
        "            toks = self.encode_message(message)\n",
        "            tokens.extend(toks)\n",
        "\n",
        "        # Add the start of an assistant message for the model to complete.\n",
        "        tokens.extend(self.encode_header(Role.assistant.value))\n",
        "\n",
        "        return ModelInput(tokens=tokens)\n",
        "\n",
        "    # TODO(this should be generic, not only for assistant messages)\n",
        "    def decode_assistant_message(\n",
        "        self, tokens: List[int], stop_reason: StopReason\n",
        "    ) -> CompletionMessage:\n",
        "        content = self.tokenizer.decode(tokens)\n",
        "        content = content.strip(\" \")\n",
        "        for _, header_str in self.possible_headers.items():\n",
        "            if content.startswith(header_str):\n",
        "                content = content[len(header_str) :]\n",
        "                break\n",
        "\n",
        "        ipython = content.startswith(\"<|python_tag|>\")\n",
        "        if ipython:\n",
        "            content = content[len(\"<|python_tag|>\") :]\n",
        "\n",
        "        eot = content.endswith(\"<|eot_id|>\")\n",
        "        if eot:\n",
        "            content = content[: -len(\"<|eot_id|>\")]\n",
        "        else:\n",
        "            content = content[: -len(\"<|eom_id|>\")]\n",
        "\n",
        "        tool_name = None\n",
        "        tool_arguments = {}\n",
        "\n",
        "        custom_tool_info = ToolUtils.maybe_extract_custom_tool_call(content)\n",
        "        if custom_tool_info is not None:\n",
        "            tool_name, tool_arguments = custom_tool_info\n",
        "            # Sometimes when agent has custom tools alongside builin tools\n",
        "            # Agent responds for builtin tool calls in the format of the custom tools\n",
        "            # This code tries to handle that case\n",
        "            if tool_name in BuiltinTool.__members__:\n",
        "                tool_name = BuiltinTool[tool_name]\n",
        "                tool_arguments = {\n",
        "                    \"query\": list(tool_arguments.values())[0],\n",
        "                }\n",
        "        else:\n",
        "            builtin_tool_info = ToolUtils.maybe_extract_builtin_tool_call(content)\n",
        "            if builtin_tool_info is not None:\n",
        "                tool_name, query = builtin_tool_info\n",
        "                tool_arguments = {\n",
        "                    \"query\": query,\n",
        "                }\n",
        "                if tool_name in BuiltinTool.__members__:\n",
        "                    tool_name = BuiltinTool[tool_name]\n",
        "            elif ipython:\n",
        "                tool_name = BuiltinTool.code_interpreter\n",
        "                tool_arguments = {\n",
        "                    \"code\": content,\n",
        "                }\n",
        "\n",
        "        tool_calls = []\n",
        "        if tool_name is not None and tool_arguments is not None:\n",
        "            call_id = str(uuid.uuid4())\n",
        "            tool_calls.append(\n",
        "                ToolCall(\n",
        "                    call_id=call_id,\n",
        "                    tool_name=tool_name,\n",
        "                    arguments=tool_arguments,\n",
        "                )\n",
        "            )\n",
        "            content = \"\"\n",
        "\n",
        "        return CompletionMessage(\n",
        "            content=content,\n",
        "            stop_reason=stop_reason,\n",
        "            tool_calls=tool_calls,\n",
        "        )"
      ],
      "metadata": {
        "id": "nGXZMzeLwuhj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-end JAX"
      ],
      "metadata": {
        "id": "hqowvT6wnti7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparams"
      ],
      "metadata": {
        "id": "QbxhYibnHdOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_dim = 4096\n",
        "params_n_layers = 32\n",
        "params_n_heads = 32\n",
        "params_n_kv_heads = 8\n",
        "params_vocab_size = 128256\n",
        "params_ffn_dim_multiplier = 1.3\n",
        "params_multiple_of = 1024\n",
        "params_norm_eps = 1e-05\n",
        "params_rope_theta = 500000.0\n",
        "params_use_scaled_rope = True\n",
        "\n",
        "params_max_batch_size = 32\n",
        "# params_max_seq_len = 2048\n",
        "params_max_seq_len = 512\n",
        "\n",
        "model_parallel_size = 1\n",
        "n_kv_heads = params_n_heads if params_n_kv_heads is None else params_n_kv_heads\n",
        "n_local_heads = params_n_heads // model_parallel_size\n",
        "n_local_kv_heads = n_kv_heads // model_parallel_size\n"
      ],
      "metadata": {
        "id": "NfsbejFiHi35"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "RzR6Hp6FHWZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flax\n",
        "\n",
        "class JaxRMSNorm(flax.linen.Module):\n",
        "    dim: int\n",
        "    eps: float\n",
        "\n",
        "    @flax.linen.compact\n",
        "    def __call__(self, x):\n",
        "        pprint_d(\"rms_norm input x\", x)\n",
        "        # print(\"JaxRMSNorm input x\", deetnosum(x))\n",
        "        weight = self.param('weight', flax.linen.initializers.ones, (self.dim,), dtype=jnp.bfloat16)\n",
        "        norm_x = x.astype(jnp.float32)\n",
        "        pprint_d(\"norm_x\", norm_x)\n",
        "        xpow2 = norm_x ** 2\n",
        "        pprint_d(\"xpow2\", xpow2)\n",
        "        xpow2mean = xpow2.mean(-1, keepdims=True)\n",
        "        pprint_d(\"xpow2mean\", xpow2mean)\n",
        "        pprint(\"self.eps\", self.eps)\n",
        "        norm_inner = xpow2mean + self.eps\n",
        "        pprint_d(\"norm_inner\", norm_inner)\n",
        "        norm_rsqrt = jax.lax.rsqrt(norm_inner)\n",
        "        pprint_d(\"norm_rsqrt\", norm_rsqrt)\n",
        "        norm_out = norm_x * norm_rsqrt\n",
        "        pprint_d(\"norm_out\", norm_out)\n",
        "        output = norm_out.astype(x.dtype)\n",
        "        pprint_d(\"weight\", weight)\n",
        "        pprint_d(\"output\", output)\n",
        "        result = output * jnp.broadcast_to(weight, output.shape)\n",
        "        pprint_d(\"result\", result)\n",
        "        return result\n",
        "\n",
        "def jax_reshape_for_broadcast(freqs_cis, x):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.reshape(*shape)\n",
        "\n",
        "\n",
        "def jax_apply_rotary_emb(\n",
        "    xq,\n",
        "    xk,\n",
        "    freqs_cis,\n",
        "):\n",
        "  pprint_d(\"jax_apply_rotary xq\", xq)\n",
        "  pprint_d(\"jax_apply_rotary xk\", xk)\n",
        "  pprint_d(\"jax_apply_rotary freqs_cis\", freqs_cis)\n",
        "  xqqq = xq.astype(jnp.float32).reshape(*xq.shape[:-1], -1, 2)\n",
        "  xkkk = xk.astype(jnp.float32).reshape(*xk.shape[:-1], -1, 2)\n",
        "  pprint_d(\"jax_apply_rotary xqqq\", xqqq)\n",
        "  pprint_d(\"jax_apply_rotary xkkk\", xkkk)\n",
        "  xq_ = xqqq.view(jnp.complex64).squeeze(-1)\n",
        "  xk_ = xkkk.view(jnp.complex64).squeeze(-1)\n",
        "  pprint_d(\"jax_apply_rotary xq_\", xq_)\n",
        "  pprint_d(\"jax_apply_rotary xk_\", xk_)\n",
        "  freqs_cis = jax_reshape_for_broadcast(freqs_cis, xq_)\n",
        "  pprint_d(\"jax_apply_rotary reshaped freqs_cis\", freqs_cis)\n",
        "  xq_outf = (xq_ * freqs_cis).view(jnp.float32)\n",
        "  xk_outf = (xk_ * freqs_cis).view(jnp.float32)\n",
        "  xq_out = xq_outf.reshape(*xq_outf.shape[:3], -1)\n",
        "  xk_out = xk_outf.reshape(*xk_outf.shape[:3], -1)\n",
        "  pprint_d(\"jax_apply_rotary xq_out\", xq_out)\n",
        "  pprint_d(\"jax_apply_rotary xk_out\", xk_out)\n",
        "  xqoo = xq_out.astype(xq.dtype)\n",
        "  xkoo = xk_out.astype(xk.dtype)\n",
        "  pprint_d(\"jax_apply_rotary xqoo\", xqoo)\n",
        "  pprint_d(\"jax_apply_rotary xkoo\", xkoo)\n",
        "  return xqoo, xkoo\n",
        "\n",
        "def jax_repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "  pprint_d(\"jax_repeat_kv x\", x)\n",
        "  pprint(\"jax_repeat_kv n_rep\", n_rep)\n",
        "  bs, slen, n_kv_heads, head_dim = x.shape\n",
        "  if n_rep == 1:\n",
        "      return x\n",
        "  out = x[:, :, :, None, :]\n",
        "  out = jnp.broadcast_to(out, (bs, slen, n_kv_heads, n_rep, head_dim))\n",
        "  out = out.reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
        "  pprint_d(\"jax_repeat_kv out\", out)\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "6fsb5NLCl0eH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "# @jax.jit\n",
        "# def dyn_set_slice(arr, start_pos, seqlen):\n",
        "#     # Compute the position where the masking should start\n",
        "#     update_start = start_pos + seqlen\n",
        "\n",
        "#     # Create a mask that is True before `update_start` and False after\n",
        "#     mask = jnp.arange(swapaxekeys.shape[-1]) < update_start  # Shape: (last_dim,)\n",
        "\n",
        "#     # Reshape mask to be broadcastable over the other dimensions\n",
        "#     mask = mask[jnp.newaxis, jnp.newaxis, jnp.newaxis, :]  # Shape: (1, 1, 1, last_dim)\n",
        "\n",
        "#     # Use jnp.where to set values to 0 where mask is False\n",
        "#     return jnp.where(mask, swapaxekeys, 0)\n",
        "\n",
        "class JaxCache(flax.linen.Module):\n",
        "  shape: Any\n",
        "\n",
        "    # is_initialized = self.has_variable('cache', 'cached_value')\n",
        "    # model_parallel_size = 1\n",
        "    # n_local_kv_heads = params_n_kv_heads // model_parallel_size\n",
        "    # head_dim = params_dim // params_n_heads\n",
        "\n",
        "    # (params_max_batch_size, params_max_seq_len, n_local_kv_heads, head_dim)\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, start_pos):\n",
        "    is_initialized = self.has_variable('cache', 'cached_value')\n",
        "    cached_value = self.variable(\n",
        "        'cache', 'cached_value', jnp.zeros, shape=self.shape, dtype=jnp.bfloat16\n",
        "    )\n",
        "    if is_initialized:\n",
        "      # pprint_d(\"before updating cache\", cached_value.value)\n",
        "      indices = (0, start_pos, 0, 0)\n",
        "      value = jax.lax.dynamic_update_slice(cached_value.value, x, indices)\n",
        "      cached_value.value = value\n",
        "      # pprint_d(\"after updating cache\", cached_value.value)\n",
        "    else:\n",
        "      print(\"INITIALIZING CACHE!!!\")\n",
        "    return cached_value.value\n",
        "\n",
        "# attention module\n",
        "\n",
        "class JaxAttention(flax.linen.Module):\n",
        "  dim: int\n",
        "  n_heads: int\n",
        "  n_kv_heads: int\n",
        "  n_local_heads: int\n",
        "  n_local_kv_heads: int\n",
        "  n_rep: int\n",
        "  head_dim: int\n",
        "  max_batch_size: int\n",
        "  max_seq_len: int\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, start_pos, freqs_cis, mask):\n",
        "    print(\"JaxAttention\")\n",
        "    bsz, seqlen, _ = x.shape\n",
        "    pprint(f\"{bsz=} {seqlen=}\")\n",
        "\n",
        "    pprint(f\"JaxAttention {start_pos=} {seqlen=}\")\n",
        "\n",
        "    pprint_d(\"jax_attn_forward layer 0 x\", x)\n",
        "\n",
        "    xq = flax.linen.Dense(features=self.n_heads * self.head_dim, use_bias=False, name=\"wq\", param_dtype=jnp.bfloat16)(x)\n",
        "    pprint_d(\"jax_pre-view xq in layer 0\", xq)\n",
        "    xk = flax.linen.Dense(features=self.n_kv_heads * self.head_dim, use_bias=False, name=\"wk\", param_dtype=jnp.bfloat16)(x)\n",
        "    pprint_d(\"jax_pre-view xk in layer 0\", xk)\n",
        "    xv = flax.linen.Dense(features=self.n_kv_heads * self.head_dim, use_bias=False, name=\"wv\", param_dtype=jnp.bfloat16)(x)\n",
        "    pprint_d(\"jax_pre-view xv in layer 0\", xv)\n",
        "\n",
        "    xq = xq.reshape((bsz, seqlen, self.n_local_heads, self.head_dim))\n",
        "    pprint_d(\"jax_initial xq in layer 0\", xq)\n",
        "    xk = xk.reshape((bsz, seqlen, self.n_local_kv_heads, self.head_dim))\n",
        "    pprint_d(\"jax_initial xk in layer 0\", xk)\n",
        "    xv = xv.reshape((bsz, seqlen, self.n_local_kv_heads, self.head_dim))\n",
        "    pprint_d(\"jax_initial xv in layer 0\", xv)\n",
        "\n",
        "\n",
        "    xq, xk = jax_apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "    pprint_d(\"jax_rotaried xq in layer 0\", xq)\n",
        "    pprint_d(\"jax_rotaried xk in layer 0\", xk)\n",
        "\n",
        "    cache_shape = (self.max_batch_size, self.max_seq_len, self.n_local_kv_heads, self.head_dim)\n",
        "    # keys = JaxCache_jit(name='keys_cache', shape=cache_shape)(x=xk, start_pos=start_pos)\n",
        "    # values = JaxCache_jit(name='values_cache', shape=cache_shape)(x=xv, start_pos=start_pos)\n",
        "    keys = JaxCache(name='keys_cache', shape=cache_shape)(x=xk, start_pos=start_pos)\n",
        "    values = JaxCache(name='values_cache', shape=cache_shape)(x=xv, start_pos=start_pos)\n",
        "    pprint_d(\"jax_cache keys\", keys)\n",
        "    pprint_d(\"jax_cache values\", values)\n",
        "\n",
        "    # keys = keys[:bsz, : start_pos + seqlen]\n",
        "    # values = values[:bsz, : start_pos + seqlen]\n",
        "    keys = keys[:bsz]\n",
        "    values = values[:bsz]\n",
        "    pprint_d(\"jax_initial keys\", keys)\n",
        "    pprint_d(\"jax_initial values\", values)\n",
        "    # keys = keys.at[:, start_pos + seqlen:].set(0)\n",
        "    # values = values.at[:, start_pos + seqlen:].set(0)\n",
        "    # pprint_d(\"jax_tozero keys\", keys)\n",
        "    # pprint_d(\"jax_tozero values\", values)\n",
        "\n",
        "    for i in range(50):\n",
        "      pprint_d(f\"keys {i=}\", keys[:, i, :, :])\n",
        "    for i in range(50):\n",
        "      pprint_d(f\"values {i=}\", values[:, i, :, :])\n",
        "# jax_initial keys ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "\n",
        "    # repeat k/v heads if n_kv_heads < n_heads\n",
        "    keys = jax_repeat_kv(\n",
        "        keys, self.n_rep\n",
        "    )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
        "    values = jax_repeat_kv(\n",
        "        values, self.n_rep\n",
        "    )  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n",
        "\n",
        "    pprint_d(\"jax_rep keys\", keys)\n",
        "    pprint_d(\"jax_rep values\", values)\n",
        "\n",
        "# jax_cache keys ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_cache values ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial keys ((1, 26, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(21245.047980487347))\n",
        "# jax_initial values ((1, 26, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(401.14362750109285))\n",
        "# jax_repeat_kv x ((1, 26, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(21245.047980487347))\n",
        "# jax_repeat_kv n_rep 4\n",
        "# jax_repeat_kv out ((1, 26, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(84980.19192194939))\n",
        "# jax_repeat_kv x ((1, 26, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(401.14362750109285))\n",
        "# jax_repeat_kv n_rep 4\n",
        "# jax_repeat_kv out ((1, 26, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(1604.5745100043714))\n",
        "# jax_rep keys ((1, 26, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(84980.19192194939))\n",
        "# jax_rep values ((1, 26, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(1604.5745100043714))\n",
        "# jax_transp xq ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(66869.51667878032))\n",
        "# jax_transp keys ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(84980.19192194939))\n",
        "# jax_transp values ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(1604.5745100043714))\n",
        "# jax_swapaxekeys ((1, 32, 128, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(84980.19192194939))\n",
        "# jax_initial scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(40094.54039424658))\n",
        "# jax_mask ((26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_masked scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_softmax scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "# jax_initial output ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(473.6378316304181))\n",
        "\n",
        "# jax_cache keys ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_cache values ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial keys ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial values ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv x ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv n_rep 4\n",
        "# jax_repeat_kv out ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv x ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv n_rep 4\n",
        "# jax_repeat_kv out ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_rep keys ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_rep values ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_transp xq ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(66869.51667878032))\n",
        "# jax_transp keys ((1, 32, 2048, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_transp values ((1, 32, 2048, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_swapaxekeys ((1, 32, 128, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_mask ((26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_masked scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_softmax scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_initial output ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_transp output ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "\n",
        "    xq = jnp.swapaxes(xq, 1, 2)\n",
        "    keys = jnp.swapaxes(keys, 1, 2)\n",
        "    values = jnp.swapaxes(values, 1, 2)\n",
        "\n",
        "# jax_cache keys ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_cache values ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial keys ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial values ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv x ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv n_rep 4\n",
        "# jax_repeat_kv out ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv x ((1, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_repeat_kv n_rep 4\n",
        "# jax_repeat_kv out ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_rep keys ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_rep values ((1, 2048, 32, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_transp xq ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(66869.51667878032))\n",
        "# jax_transp keys ((1, 32, 2048, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_transp values ((1, 32, 2048, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_swapaxekeys ((1, 32, 128, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_initial scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_mask ((26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_masked scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_masked scores sliced ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_softmax scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "\n",
        "    pprint_d(\"jax_transp xq\", xq)\n",
        "    pprint_d(\"jax_transp keys\", keys)\n",
        "    pprint_d(\"jax_transp values\", values)\n",
        "\n",
        "    swapaxekeys = jnp.swapaxes(keys, 2, 3)\n",
        "    pprint_d(\"jax_swapaxekeys\", swapaxekeys)\n",
        "# jax_swapaxekeys ((1, 32, 128, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "    # masked_swapaxekeys = swapaxekeys.at[:, :, :, start_pos + seqlen:].set(0)\n",
        "    masked_swapaxekeys = swapaxekeys\n",
        "    scores = jnp.matmul(xq, masked_swapaxekeys) / math.sqrt(self.head_dim)\n",
        "    pprint_d(\"jax_initial scores\", scores)\n",
        "    if mask is not None:\n",
        "        pprint_d(\"jax_mask\", mask)\n",
        "        # scores = scores + jnp.broadcast_to(mask, scores.shape)  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
        "        bcmask = jnp.broadcast_to(mask, scores.shape)\n",
        "        scores = jnp.where(bcmask > -1000, scores, bcmask)\n",
        "# jax_masked scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_masked scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "    pprint_d(\"jax_mask-masked scores\", scores)\n",
        "    # for i in range(start_pos + seqlen):\n",
        "    for i in range(50):\n",
        "      pprint_d(f\"jax_score {i=}\", scores[:, :, :, i])\n",
        "    # pprint_d(\"jax_mask-masked scores sliced\", scores[:, :, :, : start_pos + seqlen])\n",
        "\n",
        "# jax_alt_masked_scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_alt_scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "\n",
        "# jax_idx-masked scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_idx-masked scores slice ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_softmax scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "\n",
        "    # if True:\n",
        "    if False:\n",
        "      alt_masked_scores = scores[:, :, :, : start_pos + seqlen]\n",
        "      pprint_d(\"jax_alt_masked_scores\", alt_masked_scores)\n",
        "      alt_scores = jax.nn.softmax(alt_masked_scores.astype(jnp.float32), axis=-1).astype(xq.dtype)\n",
        "      pprint_d(\"jax_alt_scores\", alt_scores)\n",
        "    # masked_scores = scores[:, :, :, : start_pos + seqlen]\n",
        "    # masked_scores = scores.at[:, :, :, start_pos + seqlen:].set(-jnp.inf)\n",
        "    masked_scores = jnp.where(jnp.arange(scores.shape[-1]) < start_pos + seqlen, scores, -jnp.inf)\n",
        "    pprint_d(\"jax_idx-masked scores\", masked_scores)\n",
        "    # pprint_d(\"jax_idx-masked scores slice\", masked_scores[:, :, :, : start_pos + seqlen])\n",
        "    scores = jax.nn.softmax(masked_scores.astype(jnp.float32), axis=-1).astype(xq.dtype)\n",
        "    pprint_d(\"jax_softmax scores\", scores)\n",
        "# jax_transp values ((1, 32, 2048, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_transp values ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(1604.5745100043714))\n",
        "    # masked_values = values[:, :, : start_pos + seqlen, :]\n",
        "    # masked_values = values.at[:, :, start_pos + seqlen:, :].set(0)\n",
        "    masked_values = values\n",
        "    # pprint_d(\"jax_masked_values\", masked_values)\n",
        "    output = jnp.matmul(scores, masked_values)  # (bs, n_local_heads, seqlen, head_dim)\n",
        "    initial_output = output\n",
        "    pprint_d(\"jax_initial output\", output)\n",
        "    output = jnp.swapaxes(output, 1, 2)\n",
        "    output = output.reshape(bsz, seqlen, -1)\n",
        "    pprint_d(\"jax_transp output\", output)\n",
        "    wo_out = flax.linen.Dense(features=self.dim, use_bias=False, name=\"wo\", param_dtype=jnp.bfloat16)(output)\n",
        "    pprint_d(\"jax_wo_out\", wo_out)\n",
        "\n",
        "    # ll = start_pos + seqlen\n",
        "    # pprint_d(\"jax_softmax scores sliced\", scores[:, :, :, :ll])\n",
        "# jax_softmax scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_initial output ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_transp output ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_wo_out ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "\n",
        "# jax_softmax scores ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "# jax_initial output ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(473.6378316304181))\n",
        "# jax_transp output ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(473.6378316304181))\n",
        "# jax_wo_out ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(377.07218306395225))\n",
        "\n",
        "# jax_score i=47 ((1, 32, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_score i=48 ((1, 32, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_score i=49 ((1, 32, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_masked scores sliced ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_softmax scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "# jax_initial output ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_transp output ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_wo_out ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "# jax_softmax scores sliced ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "# h_attn ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(nan))\n",
        "\n",
        "# jax_masked scores sliced ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(inf))\n",
        "# jax_softmax scores ((1, 32, 26, 2048), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "# jax_initial output ((1, 32, 26, 128), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(473.6378316304181))\n",
        "# jax_transp output ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(473.6378316304181))\n",
        "# jax_wo_out ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(377.07218306395225))\n",
        "# jax_softmax scores sliced ((1, 32, 26, 26), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(832.0088773764828))\n",
        "# h_attn ((1, 26, 4096), dtype(bfloat16), {CpuDevice(id=0)}, np.float64(377.07218306395225))\n",
        "\n",
        "    return wo_out\n",
        "\n",
        "class JaxFeedForward(flax.linen.Module):\n",
        "  dim: int\n",
        "  hidden_dim: int\n",
        "  ffn_dim_multiplier: float\n",
        "  multiple_of: int\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x):\n",
        "    print(\"JaxFeedForward\")\n",
        "    dim = self.dim\n",
        "    hidden_dim = self.hidden_dim\n",
        "    ffn_dim_multiplier = self.ffn_dim_multiplier\n",
        "    multiple_of = self.multiple_of\n",
        "\n",
        "    hidden_dim = int(2 * hidden_dim / 3)\n",
        "    # custom dim factor multiplier\n",
        "    if ffn_dim_multiplier is not None:\n",
        "        hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "    w1 = flax.linen.Dense(features=hidden_dim, use_bias=False, name=\"w1\", param_dtype=jnp.bfloat16)(x)\n",
        "    w1silu = jax.nn.silu(w1)\n",
        "    w3 = flax.linen.Dense(features=hidden_dim, use_bias=False, name=\"w3\", param_dtype=jnp.bfloat16)(x)\n",
        "    inner = w1silu * w3\n",
        "    w2 = flax.linen.Dense(features=dim, use_bias=False, name=\"w2\", param_dtype=jnp.bfloat16)(inner)\n",
        "    return w2\n",
        "\n",
        "\n",
        "\n",
        "class JaxTransformerBlock(flax.linen.Module):\n",
        "  dim: int\n",
        "  attn_n_heads: int\n",
        "  attn_n_kv_heads: int\n",
        "  attn_n_local_heads: int\n",
        "  attn_n_local_kv_heads: int\n",
        "  attn_n_rep: int\n",
        "  attn_head_dim: int\n",
        "  attn_norm_eps: float\n",
        "  ffn_norm_eps: float\n",
        "  hidden_dim: int\n",
        "  ffn_dim_multiplier: float\n",
        "  multiple_of: int\n",
        "  max_batch_size: int\n",
        "  max_seq_len: int\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, x, start_pos, freqs_cis, mask):\n",
        "    print(f\"JaxTransformerBlock {start_pos=} {freqs_cis.shape=} mask.shape={mask.shape if mask is not None else None}\")\n",
        "    attn_x = JaxRMSNorm(dim=self.dim, eps=self.attn_norm_eps, name=\"attn_norm\")(x)\n",
        "    pprint_d(\"attn_x\", attn_x)\n",
        "    # print(\"attn_x\", deet(attn_x))\n",
        "    # print(\"start_pos\", start_pos)\n",
        "    # print(\"freqs_cis\", deet(freqs_cis))\n",
        "    # print(\"mask\", deet(mask) if mask is not None else None)\n",
        "    h_attn = JaxAttention(\n",
        "    # h_attn = JaxAttention_jit(\n",
        "      name=\"attn\",\n",
        "      dim=self.dim,\n",
        "      n_heads=self.attn_n_heads,\n",
        "      n_kv_heads=self.attn_n_kv_heads,\n",
        "      n_local_heads=self.attn_n_local_heads,\n",
        "      n_local_kv_heads=self.attn_n_local_kv_heads,\n",
        "      n_rep=self.attn_n_rep,\n",
        "      head_dim=self.attn_head_dim,\n",
        "      max_batch_size=self.max_batch_size,\n",
        "      max_seq_len=self.max_seq_len,\n",
        "    )(attn_x, start_pos, freqs_cis, mask)\n",
        "    pprint_d(\"h_attn\", h_attn)\n",
        "\n",
        "    h_plus_attn = x + h_attn\n",
        "    pprint_d(\"jax_block h_plus_attn\", h_plus_attn)\n",
        "    block_norm = JaxRMSNorm(dim=self.dim, eps=self.ffn_norm_eps, name=\"ffn_norm\")(h_plus_attn)\n",
        "    pprint_d(\"jax_block_norm\", block_norm)\n",
        "    block_ffn = JaxFeedForward(name=\"ffn\", dim=self.dim, hidden_dim=self.hidden_dim, ffn_dim_multiplier=self.ffn_dim_multiplier, multiple_of=self.multiple_of)(block_norm)\n",
        "    pprint_d(\"jax_block_ffn\", block_ffn)\n",
        "    block_out = h_plus_attn + block_ffn\n",
        "    pprint_d(\"jax_block_out\", block_out)\n",
        "    return block_out\n"
      ],
      "metadata": {
        "id": "CLOx9WespN8b"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer module"
      ],
      "metadata": {
        "id": "y-0eKR5VGWyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer module\n",
        "\n",
        "def jax_forward_mask(total_len, seqlen, dtype, device):\n",
        "  # First start_pos columns of all-0s, then a triangular -inf in upper right.\n",
        "  # I.e. similar to skipping the first start_pos rows(?)\n",
        "  mask = jnp.full((seqlen, total_len), float(\"-inf\"), device=device)\n",
        "  mask = jnp.triu(mask, k=1)\n",
        "  # mask = jnp.hstack([\n",
        "  #     jnp.zeros((seqlen, start_pos)),\n",
        "  #     mask\n",
        "  # ])\n",
        "  mask = mask.astype(dtype)\n",
        "  pprint_d(\"mask\", mask)\n",
        "  return mask\n",
        "\n",
        "class JaxTransformer(flax.linen.Module):\n",
        "  dim: int\n",
        "  attn_n_heads: int\n",
        "  attn_n_kv_heads: int\n",
        "  attn_n_local_heads: int\n",
        "  attn_n_local_kv_heads: int\n",
        "  attn_n_rep: int\n",
        "  attn_head_dim: int\n",
        "  attn_norm_eps: float\n",
        "  ffn_norm_eps: float\n",
        "  output_norm_eps: float\n",
        "  hidden_dim: int\n",
        "  ffn_dim_multiplier: float\n",
        "  multiple_of: int\n",
        "  max_batch_size: int\n",
        "  max_seq_len: int\n",
        "  vocab_size: int\n",
        "  is_prefilling: bool\n",
        "  prefill_len: int\n",
        "  device: bool\n",
        "\n",
        "  @flax.linen.compact\n",
        "  def __call__(self, tokens, start_pos):\n",
        "    print(f\"JaxTransformer {tokens.shape=} {start_pos=} {self.is_prefilling=}\")\n",
        "    print(\"tokens\", deetnodev(tokens))\n",
        "    _bsz, seqlen = tokens.shape\n",
        "    h = flax.linen.Embed(\n",
        "      num_embeddings=self.vocab_size,\n",
        "      features=self.dim,\n",
        "      name=\"tok_embeddings\",\n",
        "      param_dtype=jnp.bfloat16,\n",
        "    )(tokens)\n",
        "    if self.is_prefilling:\n",
        "      freqs_cis = jax_freqs_cis[:self.prefill_len]\n",
        "    else:\n",
        "      # freqs_cis = jax_freqs_cis[start_pos : start_pos + seqlen]\n",
        "      # print(\"freqs_cis\", deet(freqs_cis))\n",
        "      # freqs_cis_slice = jnp.expand_dims(jax_freqs_cis[start_pos], axis=0)\n",
        "      # print(\"freqs_cis_slice\", deet(freqs_cis_slice))\n",
        "      # freqs_cis = freqs_cis_slice\n",
        "      freqs_cis = jnp.expand_dims(jax_freqs_cis[start_pos], axis=0)\n",
        "    print(f\"  freqs_cis {freqs_cis.shape=} {start_pos=} {seqlen=}\")\n",
        "\n",
        "    model_parallel_size = 1\n",
        "    n_kv_heads = params_n_heads if params_n_kv_heads is None else params_n_kv_heads\n",
        "    n_local_heads = params_n_heads // model_parallel_size\n",
        "    n_local_kv_heads = n_kv_heads // model_parallel_size\n",
        "\n",
        "    mask = None\n",
        "    if seqlen > 1:\n",
        "      # tokdev = tokens.devices()\n",
        "      # if len(tokdev) == 1:\n",
        "        # mask = jax_forward_mask(seqlen, start_pos, h.dtype, list(tokdev)[0])\n",
        "        # mask = jax_forward_mask(start_pos + seqlen, seqlen, start_pos, h.dtype, list(tokdev)[0])\n",
        "        mask = jax_forward_mask(self.max_seq_len, seqlen, h.dtype, self.device)\n",
        "        print(f\"  mask {mask.shape=} {self.max_seq_len=} {seqlen=}\")\n",
        "      # else:\n",
        "      #   print(\"UNEXPECTED TOKEN DEVICES\", deet(tokens))\n",
        "      #   raise Exception('tokens not on a single device!')\n",
        "\n",
        "    pprint(f\"seqlen\", seqlen)\n",
        "    pprint_d(f\"start h\", h)\n",
        "    pprint(f\"start_pos\", start_pos)\n",
        "    pprint_d(f\"freqs_cis\", freqs_cis)\n",
        "    if mask is None:\n",
        "      pprint(\"mask\", mask)\n",
        "    else:\n",
        "      pprint_d(f\"mask\", mask)\n",
        "\n",
        "    for n in range(params_n_layers):\n",
        "      pprint(f\"doing layer layer{n}\")\n",
        "      # enable_pprint(False)\n",
        "      h = JaxTransformerBlock(\n",
        "        name=f\"layer{n}\",\n",
        "        dim=params_dim,\n",
        "        attn_n_heads=params_n_heads,\n",
        "        attn_n_kv_heads=n_kv_heads,\n",
        "        attn_n_local_heads=n_local_heads,\n",
        "        attn_n_local_kv_heads=n_local_kv_heads,\n",
        "        attn_n_rep = n_local_heads // n_local_kv_heads,\n",
        "        attn_head_dim = params_dim // params_n_heads,\n",
        "        attn_norm_eps=params_norm_eps,\n",
        "        ffn_norm_eps=params_norm_eps,\n",
        "        hidden_dim=params_dim*4,\n",
        "        ffn_dim_multiplier=params_ffn_dim_multiplier,\n",
        "        multiple_of=params_multiple_of,\n",
        "        max_batch_size=params_max_batch_size,\n",
        "        max_seq_len=params_max_seq_len,\n",
        "      )(x=h, start_pos=start_pos, freqs_cis=freqs_cis, mask=mask)\n",
        "      pprint_d(f\"layer {n} h\", h)\n",
        "      enable_pprint(False)\n",
        "      # enable_pprint(True)\n",
        "    pprint(\"doing output\")\n",
        "    h = JaxRMSNorm(dim=self.dim, eps=self.output_norm_eps, name=\"output_norm\")(h)\n",
        "    output = flax.linen.Dense(features=self.vocab_size, use_bias=False, name=\"output\", param_dtype=jnp.bfloat16)(h)\n",
        "    output = output.astype(jnp.float32)\n",
        "    return output\n",
        "\n",
        "jax_transformer_prefill = JaxTransformer(\n",
        "    dim=params_dim,\n",
        "    attn_n_heads=params_n_heads,\n",
        "    attn_n_kv_heads=n_kv_heads,\n",
        "    attn_n_local_heads=n_local_heads,\n",
        "    attn_n_local_kv_heads=n_local_kv_heads,\n",
        "    attn_n_rep = n_local_heads // n_local_kv_heads,\n",
        "    attn_head_dim = params_dim // params_n_heads,\n",
        "    attn_norm_eps=params_norm_eps,\n",
        "    ffn_norm_eps=params_norm_eps,\n",
        "    output_norm_eps=params_norm_eps,\n",
        "    hidden_dim=params_dim*4,\n",
        "    ffn_dim_multiplier=params_ffn_dim_multiplier,\n",
        "    multiple_of=params_multiple_of,\n",
        "    max_batch_size=params_max_batch_size,\n",
        "    max_seq_len=params_max_seq_len,\n",
        "    vocab_size=params_vocab_size,\n",
        "    is_prefilling=True,\n",
        "    prefill_len=256,\n",
        "    device=jax_tpu(),\n",
        ")\n",
        "\n",
        "print(\"jax_transformer_prefill\", jax_transformer_prefill)\n",
        "\n",
        "jax_transformer_incremental = JaxTransformer(\n",
        "    dim=params_dim,\n",
        "    attn_n_heads=params_n_heads,\n",
        "    attn_n_kv_heads=n_kv_heads,\n",
        "    attn_n_local_heads=n_local_heads,\n",
        "    attn_n_local_kv_heads=n_local_kv_heads,\n",
        "    attn_n_rep = n_local_heads // n_local_kv_heads,\n",
        "    attn_head_dim = params_dim // params_n_heads,\n",
        "    attn_norm_eps=params_norm_eps,\n",
        "    ffn_norm_eps=params_norm_eps,\n",
        "    output_norm_eps=params_norm_eps,\n",
        "    hidden_dim=params_dim*4,\n",
        "    ffn_dim_multiplier=params_ffn_dim_multiplier,\n",
        "    multiple_of=params_multiple_of,\n",
        "    max_batch_size=params_max_batch_size,\n",
        "    max_seq_len=params_max_seq_len,\n",
        "    vocab_size=params_vocab_size,\n",
        "    is_prefilling=False,\n",
        "    prefill_len=0,\n",
        "    device=jax_tpu(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "55rlV1tThyIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ac2f14-b76e-457e-f05e-1518544486fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax_transformer_prefill JaxTransformer(\n",
            "    # attributes\n",
            "    dim = 4096\n",
            "    attn_n_heads = 32\n",
            "    attn_n_kv_heads = 8\n",
            "    attn_n_local_heads = 32\n",
            "    attn_n_local_kv_heads = 8\n",
            "    attn_n_rep = 4\n",
            "    attn_head_dim = 128\n",
            "    attn_norm_eps = 1e-05\n",
            "    ffn_norm_eps = 1e-05\n",
            "    output_norm_eps = 1e-05\n",
            "    hidden_dim = 16384\n",
            "    ffn_dim_multiplier = 1.3\n",
            "    multiple_of = 1024\n",
            "    max_batch_size = 32\n",
            "    max_seq_len = 512\n",
            "    vocab_size = 128256\n",
            "    is_prefilling = True\n",
            "    prefill_len = 256\n",
            "    device = TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup/load model params"
      ],
      "metadata": {
        "id": "m_198F1iGKoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jax_transformer_params {\n",
        "#     'cache': {\n",
        "#         'layer0': {'attn': {'keys_cache': {'cached_value': ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)})}, 'values_cache': {'cached_value': ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)})}}},\n",
        "#         'layer1': {'attn': {'keys_cache': {'cached_value': ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)})}, 'values_cache': {'cached_value': ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)})}}},\n",
        "#         ...\n",
        "#         'layer31': {'attn': {'keys_cache': {'cached_value': ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)})}, 'values_cache': {'cached_value': ((32, 2048, 8, 128), dtype(bfloat16), {CpuDevice(id=0)})}}},\n",
        "#     },\n",
        "#     'params': {\n",
        "#         'layer0': {'attn': {'wk': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}, 'wo': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wq': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wv': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}}, 'attn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}, 'ffn': {'w1': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}, 'w2': {'kernel': ((14336, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'w3': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}}, 'ffn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}},\n",
        "#         'layer1': {'attn': {'wk': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}, 'wo': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wq': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wv': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}}, 'attn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}, 'ffn': {'w1': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}, 'w2': {'kernel': ((14336, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'w3': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}}, 'ffn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}},\n",
        "#         ...\n",
        "#         'layer30': {'attn': {'wk': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}, 'wo': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wq': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wv': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}}, 'attn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}, 'ffn': {'w1': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}, 'w2': {'kernel': ((14336, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'w3': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}}, 'ffn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}},\n",
        "#         'layer31': {'attn': {'wk': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}, 'wo': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wq': {'kernel': ((4096, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'wv': {'kernel': ((4096, 1024), dtype(bfloat16), {CpuDevice(id=0)})}}, 'attn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}, 'ffn': {'w1': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}, 'w2': {'kernel': ((14336, 4096), dtype(bfloat16), {CpuDevice(id=0)})}, 'w3': {'kernel': ((4096, 14336), dtype(bfloat16), {CpuDevice(id=0)})}}, 'ffn_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}},\n",
        "#         'output': {'kernel': ((4096, 128256), dtype(bfloat16), {CpuDevice(id=0)})},\n",
        "#         'output_norm': {'weight': ((4096,), dtype(bfloat16), {CpuDevice(id=0)})}\n",
        "#     }\n",
        "# }\n",
        "\n",
        "def neginf(shape, dtype, device):\n",
        "  return jnp.full(shape, float(\"-inf\"), dtype=dtype, device=device)\n",
        "\n",
        "def init_empty_cache(device):\n",
        "  cache = {}\n",
        "  for n in range(params_n_layers):\n",
        "    cache[f\"layer{n}\"] = {\n",
        "        'attn': {\n",
        "            # 'keys_cache': {'cached_value': neginf((32, 2048, 8, 128), dtype=jnp.bfloat16, device=device) },\n",
        "            # 'values_cache': {'cached_value': neginf((32, 2048, 8, 128), dtype=jnp.bfloat16, device=device) },\n",
        "            'keys_cache': {'cached_value': jnp.zeros((32, params_max_seq_len, 8, 128), dtype=jnp.bfloat16, device=device) },\n",
        "            'values_cache': {'cached_value': jnp.zeros((32, params_max_seq_len, 8, 128), dtype=jnp.bfloat16, device=device) },\n",
        "        },\n",
        "    }\n",
        "  return cache\n",
        "\n",
        "# jax_layer0_params['params']['attn']['wq']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.attention.wq.weight').T\n",
        "# jax_layer0_params['params']['attn']['wk']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.attention.wk.weight').T\n",
        "# jax_layer0_params['params']['attn']['wv']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.attention.wv.weight').T\n",
        "# jax_layer0_params['params']['attn']['wo']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.attention.wo.weight').T\n",
        "\n",
        "# jax_layer0_params['params']['attn_norm']['weight'] = load_torch_weights(\"/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.attention_norm.weight\")\n",
        "\n",
        "# jax_layer0_params['params']['ffn']['w1']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.feed_forward.w1.weight').T\n",
        "# jax_layer0_params['params']['ffn']['w2']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.feed_forward.w2.weight').T\n",
        "# jax_layer0_params['params']['ffn']['w3']['kernel'] = load_torch_weights('/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.feed_forward.w3.weight').T\n",
        "\n",
        "# jax_layer0_params['params']['ffn_norm']['weight'] = load_torch_weights(\"/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/consolidated.00.pth/layers.0.ffn_norm.weight\")\n",
        "\n",
        "def load_llama3_1_params(device=None):\n",
        "  prefix = f\"/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split\"\n",
        "  params = {\n",
        "    'tok_embeddings': {'embedding': load_torch_weights(f'{prefix}/consolidated.00.pth/tok_embeddings.weight', device)},\n",
        "    'output': {'kernel': load_torch_weights(f'{prefix}/consolidated.00.pth/output.weight', device).T},\n",
        "    'output_norm': {'weight': load_torch_weights(f'{prefix}/consolidated.00.pth/norm.weight', device)},\n",
        "  }\n",
        "  for n in range(params_n_layers):\n",
        "    lprefix = f\"{prefix}/consolidated.00.pth/layers.{n}\"\n",
        "    params[f\"layer{n}\"] = {\n",
        "        'attn': {\n",
        "            'wq': {'kernel': load_torch_weights(f'{lprefix}.attention.wq.weight', device).T},\n",
        "            'wk': {'kernel': load_torch_weights(f'{lprefix}.attention.wk.weight', device).T},\n",
        "            'wv': {'kernel': load_torch_weights(f'{lprefix}.attention.wv.weight', device).T},\n",
        "            'wo': {'kernel': load_torch_weights(f'{lprefix}.attention.wo.weight', device).T},\n",
        "        },\n",
        "        'attn_norm': {'weight': load_torch_weights(f'{lprefix}.attention_norm.weight', device)},\n",
        "        'ffn': {\n",
        "            'w1': {'kernel': load_torch_weights(f'{lprefix}.feed_forward.w1.weight', device).T},\n",
        "            'w2': {'kernel': load_torch_weights(f'{lprefix}.feed_forward.w2.weight', device).T},\n",
        "            'w3': {'kernel': load_torch_weights(f'{lprefix}.feed_forward.w3.weight', device).T},\n",
        "        },\n",
        "        'ffn_norm': {'weight': load_torch_weights(f'{lprefix}.ffn_norm.weight', device)},\n",
        "    }\n",
        "  return params\n",
        "\n",
        "jax_transformer_params_tpu = load_llama3_1_params(jax_tpu())\n",
        "\n",
        "# def f(device=None):\n",
        "#   device = device or jax_cpu()\n",
        "#   with jax.default_device(device):\n",
        "#     pprint(\"initing cache...\")\n",
        "#     cache = init_empty_cache(device)\n",
        "#     pprint(\"loading params...\")\n",
        "#     params = load_llama3_1_params(device)\n",
        "#     jax_transformer_params = {\n",
        "#         'cache': cache,\n",
        "#         'params': params,\n",
        "#     }\n",
        "#     return jax_transformer_params\n",
        "\n",
        "# jax_transformer_params = None\n",
        "# jax_transformer_params_tpu = None\n",
        "\n",
        "# jax_transformer_params = f()\n",
        "\n",
        "# print(\"jax_transformer_params\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), jax_transformer_params))\n",
        "\n",
        "# jax_transformer_params_tpu = f(jax.devices(\"tpu\")[0])\n",
        "\n",
        "# print(\"jax_transformer_params_tpu\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), jax_transformer_params_tpu))\n"
      ],
      "metadata": {
        "id": "iUv_5qmyCvBa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(\"/mnt/ramdisk/llama3_1/Meta-Llama-3.1-8B-Instruct_split/tokenizer.model\")\n",
        "chat_format = ChatFormat(tokenizer)\n",
        "\n",
        "def jax_gen_tokens(model_input, max_seq_len, max_gen_len):\n",
        "  prompt_tokens = [model_input.tokens]\n",
        "  bsz = 1\n",
        "\n",
        "  pad_id = tokenizer.pad_id\n",
        "\n",
        "  min_prompt_len = min(len(t) for t in prompt_tokens)\n",
        "  max_prompt_len = max(len(t) for t in prompt_tokens)\n",
        "  total_len = min(max_gen_len + max_prompt_len, max_seq_len)\n",
        "  eos_reached = jnp.array([False] * bsz)\n",
        "\n",
        "  pprint(f\"{pad_id=}\")\n",
        "  jax_tokens = jnp.full((1, total_len), pad_id, dtype=jnp.int32)\n",
        "  for k, t in enumerate(prompt_tokens):\n",
        "    pprint(f\"{k=}, {t=}\")\n",
        "    jax_tokens = jax_tokens.at[k, :len(t)].set(jnp.array(t, dtype=jnp.int32))\n",
        "\n",
        "  pprint(\"jax_tokens\", deet(jax_tokens), jax_tokens)\n",
        "  return jax_tokens"
      ],
      "metadata": {
        "id": "dvEnwTZWJZcy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def torch_apply_scaling(freqs: torch.Tensor):\n",
        "    # Values obtained from grid search\n",
        "    scale_factor = 8\n",
        "    low_freq_factor = 1\n",
        "    high_freq_factor = 4\n",
        "    old_context_len = 8192  # original llama3 length\n",
        "\n",
        "    low_freq_wavelen = old_context_len / low_freq_factor\n",
        "    high_freq_wavelen = old_context_len / high_freq_factor\n",
        "    new_freqs = []\n",
        "    for freq in freqs:\n",
        "        wavelen = 2 * math.pi / freq\n",
        "        if wavelen < high_freq_wavelen:\n",
        "            new_freqs.append(freq)\n",
        "        elif wavelen > low_freq_wavelen:\n",
        "            new_freqs.append(freq / scale_factor)\n",
        "        else:\n",
        "            assert low_freq_wavelen != high_freq_wavelen\n",
        "            smooth = (old_context_len / wavelen - low_freq_factor) / (\n",
        "                high_freq_factor - low_freq_factor\n",
        "            )\n",
        "            new_freqs.append((1 - smooth) * freq / scale_factor + smooth * freq)\n",
        "    return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n",
        "\n",
        "def torch_precompute_freqs_cis(\n",
        "    dim: int, end: int, theta: float = 10000.0, use_scaled: bool = False\n",
        "):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    if use_scaled:\n",
        "        freqs = torch_apply_scaling(freqs)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis\n",
        "\n",
        "torch_freqs_cis = torch_precompute_freqs_cis(\n",
        "    params_dim // params_n_heads,\n",
        "    params_max_seq_len * 2,\n",
        "    params_rope_theta,\n",
        "    params_use_scaled_rope,\n",
        ")\n",
        "\n",
        "print(deets(torch_freqs_cis))\n",
        "jax_freqs_cis = torch_to_jax(torch_freqs_cis, jax_tpu())\n",
        "print(deet(jax_freqs_cis))"
      ],
      "metadata": {
        "id": "YTYpz1RdRbiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deef1b66-bdfe-4782-9fd9-26e2704dd4e9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(torch.Size([1024, 64]), torch.complex64, device(type='cpu'), np.float64(53830.88144241154))\n",
            "((1024, 64), dtype('complex64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}, np.float64(53830.88144241154))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_30873/3676075656.py:13: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)\n",
            "  x = t.to(dtype=torch.float64).detach().numpy()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"jax_transformer_params\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), jax_transformer_params))\n",
        "\n",
        "def f():\n",
        "  max_seq_len = 400\n",
        "  max_gen_len = 30\n",
        "  messages = [\n",
        "    SystemMessage(\n",
        "      content=\"This is a test sentence.\",\n",
        "    ),\n",
        "    UserMessage(\n",
        "      content=\"This is a response.\",\n",
        "    ),\n",
        "  ]\n",
        "  model_input = chat_format.encode_dialog_prompt(messages)\n",
        "  print(model_input.tokens)\n",
        "\n",
        "  jax_tokens = jax_gen_tokens(model_input, max_seq_len, max_gen_len)\n",
        "  out, new_params = jax_transformer.apply(jax_transformer_params, tokens=jax_tokens, start_pos=0, mutable=['cache'])\n",
        "  print(\"out\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), out))\n",
        "  print(\"new_params keys\", new_params.keys())\n",
        "  print(\"new_params\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), new_params))\n",
        "\n",
        "# f()"
      ],
      "metadata": {
        "id": "lHSr5PWhiz_H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "IWA46_2mXdA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def prefill_run(run_params, tokens, input_tokens_len):\n",
        "  logits, new_cache = jax_transformer_prefill.apply(run_params, tokens=tokens, start_pos=0, mutable=['cache'])\n",
        "  for k in new_cache['cache'].keys():\n",
        "    idxs = jnp.arange(512) >= input_tokens_len - 1\n",
        "    idxs = idxs[jnp.newaxis, :, jnp.newaxis, jnp.newaxis]\n",
        "    new_cache['cache'][k]['attn']['keys_cache']['cached_value'] = jnp.where(idxs, 0, new_cache['cache'][k]['attn']['keys_cache']['cached_value'])\n",
        "    new_cache['cache'][k]['attn']['values_cache']['cached_value'] = jnp.where(idxs, 0, new_cache['cache'][k]['attn']['values_cache']['cached_value'])\n",
        "  return new_cache['cache']\n",
        "\n",
        "def prefill(messages, device):\n",
        "  # enable_pprint(True)\n",
        "  model_input = chat_format.encode_dialog_prompt(messages)\n",
        "  # pprint(f\"{model_input.tokens=}\")\n",
        "  prompt_tokens = [model_input.tokens]\n",
        "  input_tokens_len = len(model_input.tokens)\n",
        "  total_len = 256\n",
        "\n",
        "  with jax.default_device(device):\n",
        "    tokens = jnp.full((1, total_len), tokenizer.pad_id, dtype=jnp.int32)\n",
        "    for k, t in enumerate(prompt_tokens):\n",
        "      tokens = tokens.at[k, :len(t)].set(jnp.array(t, dtype=jnp.int32))\n",
        "\n",
        "  # pprint_d(\"prefill tokens\", tokens)\n",
        "\n",
        "  prev_pos = 0\n",
        "\n",
        "  run_params = {'params': jax_transformer_params_tpu, 'cache': init_empty_cache(device)}\n",
        "  # pprint(\"prefill run_params\", jax.tree_util.tree_map(deetnosum, run_params))\n",
        "\n",
        "  # pprint(\"prefilling cache from prompt...\")\n",
        "  enable_pprint(False)\n",
        "  new_cache = prefill_run(run_params, tokens, input_tokens_len)\n",
        "  # print(\"prefill cache output\", jax.tree_util.tree_map(deetnosum, new_cache))\n",
        "\n",
        "  # for i in range(30):\n",
        "  #   print(f\"layer0 keys_cache[:,{i},:,:]\", deet(new_cache['layer0']['attn']['keys_cache']['cached_value'][0,i,:,:]))\n",
        "  # for i in range(30):\n",
        "  #   print(f\"layer0 valus_cache[:,{i},:,:]\", deet(new_cache['layer0']['attn']['values_cache']['cached_value'][0,i,:,:]))\n",
        "\n",
        "  return new_cache\n",
        "\n",
        "prefill(messages=[\n",
        "  SystemMessage(\n",
        "    content=\"This is a test sentence.\",\n",
        "  ),\n",
        "  UserMessage(\n",
        "    content=\"This is a response.\",\n",
        "  ),\n",
        "], device=jax_tpu())\n",
        "\n",
        "dump_mem()"
      ],
      "metadata": {
        "id": "GBFUsZg24r5m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f184d76-bed1-4aeb-e463-79ea0ab843ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JaxTransformer tokens.shape=(1, 256) start_pos=0 self.is_prefilling=True\n",
            "tokens ((1, 256), dtype('int32'))\n",
            "  freqs_cis freqs_cis.shape=(256, 64) start_pos=0 seqlen=256\n",
            "  mask mask.shape=(256, 512) self.max_seq_len=512 seqlen=256\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=0 freqs_cis.shape=(256, 64) mask.shape=(256, 512)\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "None 1 0M\n",
            "freqs_cis 1 0M\n",
            "params 291 16060M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump_mem()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlkX-vsD3JQW",
        "outputId": "3760d3a7-131e-414f-fa8a-430c85f09550"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None 1 0M\n",
            "freqs_cis 1 0M\n",
            "params 291 16060M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefill(messages=[\n",
        "  SystemMessage(\n",
        "    content=\"This is a test sentence.\",\n",
        "  ),\n",
        "  UserMessage(\n",
        "    content=\"This is a response.\",\n",
        "  ),\n",
        "], device=jax_tpu())\n",
        "\n",
        "dump_mem()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd8oBOqP46RY",
        "outputId": "b6ddbfba-f7bf-4775-ee8b-cd5ae307e2de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None 1 0M\n",
            "freqs_cis 1 0M\n",
            "params 291 16060M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump_mem()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPzv1z_NjMII",
        "outputId": "272a42b3-68bc-46b4-c711-5fc2c588941a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None 1 0M\n",
            "freqs_cis 1 0M\n",
            "params 291 16060M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def incremental_run(run_params, tokens, start_pos):\n",
        "  logits, updates = jax_transformer_incremental.apply(run_params, tokens, start_pos=start_pos, mutable=['cache'])\n",
        "  return logits, updates['cache']\n",
        "\n",
        "def f():\n",
        "  cache = prefill(messages=[\n",
        "    SystemMessage(\n",
        "      content=\"This is a test sentence.\",\n",
        "    ),\n",
        "    UserMessage(\n",
        "      content=\"This is a response.\",\n",
        "    ),\n",
        "  ], device=jax_tpu())\n",
        "  incremental_run({'params': jax_transformer_params_tpu, 'cache': cache}, tokens=jnp.zeros((1,1), dtype=jnp.int32), start_pos=10)\n",
        "\n",
        "f()"
      ],
      "metadata": {
        "id": "GOs9ofcw5-qB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395239c6-e84d-4662-d6e4-66e2714f22ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JaxTransformer tokens.shape=(1, 1) start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> self.is_prefilling=False\n",
            "tokens ((1, 1), dtype('int32'))\n",
            "  freqs_cis freqs_cis.shape=(1, 64) start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> seqlen=1\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n",
            "JaxTransformerBlock start_pos=Traced<ShapedArray(int64[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)> freqs_cis.shape=(1, 64) mask.shape=None\n",
            "JaxAttention\n",
            "JaxFeedForward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TokenResult:\n",
        "    token: int\n",
        "    text: str\n",
        "    logprobs: Optional[List[float]] = None\n",
        "\n",
        "# def sample_top_p(probs, p):\n",
        "#     \"\"\"\n",
        "#     Perform top-p (nucleus) sampling on a probability distribution.\n",
        "\n",
        "#     Args:\n",
        "#         probs (torch.Tensor): Probability distribution tensor.\n",
        "#         p (float): Probability threshold for top-p sampling.\n",
        "\n",
        "#     Returns:\n",
        "#         torch.Tensor: Sampled token indices.\n",
        "\n",
        "#     Note:\n",
        "#         Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
        "#         exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
        "#     \"\"\"\n",
        "#     probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "#     probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "#     mask = probs_sum - probs_sort > p\n",
        "#     probs_sort[mask] = 0.0\n",
        "#     probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "#     next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "#     next_token = torch.gather(probs_idx, -1, next_token)\n",
        "#     return next_token\n",
        "\n",
        "def run():\n",
        "  enable_pprint(True)\n",
        "  max_seq_len = 400\n",
        "  max_gen_len = 30\n",
        "  messages = [\n",
        "    SystemMessage(\n",
        "      content=\"This is a test sentence.\",\n",
        "    ),\n",
        "    UserMessage(\n",
        "      content=\"This is a response.\",\n",
        "    ),\n",
        "  ]\n",
        "  model_input = chat_format.encode_dialog_prompt(messages)\n",
        "  # pprint(f\"{model_input.tokens=}\")\n",
        "\n",
        "  # temperature = 0.7\n",
        "  # top_p = 0.9\n",
        "  temperature = 0\n",
        "  top_p = 0\n",
        "  logprobs = False\n",
        "  device = jax.devices()[0]\n",
        "\n",
        "  max_seq_len = 400\n",
        "  max_gen_len = 30\n",
        "  # max_gen_len = 7\n",
        "\n",
        "  prompt_tokens = [model_input.tokens]\n",
        "  bsz = 1\n",
        "\n",
        "  pad_id = tokenizer.pad_id\n",
        "\n",
        "  min_prompt_len = min(len(t) for t in prompt_tokens)\n",
        "  max_prompt_len = max(len(t) for t in prompt_tokens)\n",
        "  total_len = min(max_gen_len + max_prompt_len, max_seq_len)\n",
        "  eos_reached = jnp.array([False] * bsz)\n",
        "\n",
        "  with jax.default_device(device):\n",
        "    tokens = jnp.full((1, total_len), pad_id, dtype=jnp.int32)\n",
        "    for k, t in enumerate(prompt_tokens):\n",
        "      tokens = tokens.at[k, :len(t)].set(jnp.array(t, dtype=jnp.int32))\n",
        "    stop_tokens = jnp.array(tokenizer.stop_tokens)\n",
        "\n",
        "  # pprint_d(\"tokens\", tokens)\n",
        "\n",
        "  prev_pos = 0\n",
        "\n",
        "  input_text_mask = tokens != pad_id\n",
        "\n",
        "  # pprint_d(\"stop_tokens\", stop_tokens)\n",
        "\n",
        "  out_tokens = []\n",
        "  # run_params = jax_transformer_params_tpu\n",
        "\n",
        "  # pprint(\"run_params\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), run_params))\n",
        "  # run_params = {'params': run_params['params'], 'cache': init_empty_cache(jax_cpu())}\n",
        "  # blah()\n",
        "\n",
        "  # run_params = {'params': jax_transformer_params_tpu, 'cache': init_empty_cache(device)}\n",
        "\n",
        "  if True:\n",
        "    # pprint(\"Starting LLM...\")\n",
        "    pprint(\"Filling cache from prompt...\")\n",
        "    enable_pprint(False)\n",
        "    cache = prefill(messages, device=jax_tpu())\n",
        "    # print(\"cache\", jax.tree_util.tree_map(lambda t: (t.shape, t.dtype, t.devices()), cache))\n",
        "    enable_pprint(True)\n",
        "    prev_pos = min_prompt_len - 1\n",
        "\n",
        "    pprint(\"Generating tokens...\")\n",
        "    for cur_pos in range(min_prompt_len, total_len):\n",
        "      # pprint(\"cur_pos\", cur_pos)\n",
        "      enable_pprint(False)\n",
        "      # print(\"tokens\", deetnosum(tokens[:, prev_pos:cur_pos]))\n",
        "      # print(\"startpos\", prev_pos)\n",
        "      logits, cache = incremental_run({'params': jax_transformer_params_tpu, 'cache': cache}, tokens=tokens[:, prev_pos:cur_pos], start_pos=prev_pos)\n",
        "\n",
        "      # if cur_pos == 28:\n",
        "      #   print(f\"{prev_pos=} {cur_pos=}\")\n",
        "      #   for i in range(30):\n",
        "      #     print(f\"layer0 keys_cache[:,{i},:,:]\", deet(cache['layer0']['attn']['keys_cache']['cached_value'][0,i,:,:]))\n",
        "      #   for i in range(30):\n",
        "      #     print(f\"layer0 valus_cache[:,{i},:,:]\", deet(cache['layer0']['attn']['values_cache']['cached_value'][0,i,:,:]))\n",
        "\n",
        "      enable_pprint(True)\n",
        "      # pprint_d(\"logits\", logits)\n",
        "\n",
        "      if temperature > 0:\n",
        "          # probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "          # next_token = sample_top_p(probs, top_p)\n",
        "          pass\n",
        "      else:\n",
        "          next_token = jnp.argmax(logits[:, -1], axis=-1).astype(jnp.int32)\n",
        "          # pprint_d(\"next_token\", next_token)\n",
        "          # pprint_d(\"next_token idxed\", logits[:, -1, next_token])\n",
        "\n",
        "      next_token = next_token.reshape(-1)\n",
        "      # only replace token if prompt has already been generated\n",
        "      next_token = jnp.where(\n",
        "          input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
        "      )\n",
        "      tokens = tokens.at[:, cur_pos].set(next_token)\n",
        "\n",
        "      target = tokens[:, prev_pos + 1 : cur_pos + 1]\n",
        "      if logprobs:\n",
        "          # token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
        "          #     input=logits.transpose(1, 2),\n",
        "          #     target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
        "          #     reduction=\"none\",\n",
        "          #     ignore_index=pad_id,\n",
        "          # )\n",
        "          pass\n",
        "      eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
        "          jnp.isin(next_token, stop_tokens)\n",
        "      )\n",
        "      # pprint(\"eos_reached\", eos_reached)\n",
        "      tk = TokenResult(\n",
        "          token=int(next_token[0]),\n",
        "          text=tokenizer.decode(next_token.tolist()),\n",
        "          logprobs=(\n",
        "              token_logprobs[:, prev_pos + 1 : cur_pos + 1][0].tolist()\n",
        "              if logprobs\n",
        "              else None\n",
        "          ),\n",
        "      )\n",
        "      # print(\"tk\", tk)\n",
        "      out_tokens.append(tk)\n",
        "\n",
        "      prev_pos = cur_pos\n",
        "      if all(eos_reached):\n",
        "        break\n",
        "  print(\"\".join([t.text for t in out_tokens]))\n",
        "  return out_tokens\n",
        "\n",
        "def f(times):\n",
        "  for n in range(times):\n",
        "    print(\"moo!\", n)\n",
        "    out_tokens = run()\n",
        "\n",
        "f(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLLtlyqXUWmz",
        "outputId": "8216904c-9aeb-41d9-8884-9fe68aea1c8d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moo! 0\n",
            "Filling cache from prompt...\n",
            "Generating tokens...\n",
            "It looks like we're having a conversation! What's next?<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dump_mem()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dtnKp235WMA",
        "outputId": "f31643be-f2dd-440d-f32d-81dfa943b35f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None 1 0M\n",
            "freqs_cis 1 0M\n",
            "params 291 16060M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory debugging"
      ],
      "metadata": {
        "id": "m2E8XJpJmqc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.profiler.save_device_memory_profile(\"tpu-memory-prefill200fail.prof\", \"tpu\")"
      ],
      "metadata": {
        "id": "rqZDgN91JPgG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.profiler.save_device_memory_profile(\"cpu-memory.prof\", \"cpu\")"
      ],
      "metadata": {
        "id": "uLUm2jfMJXNH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.clear_caches()"
      ],
      "metadata": {
        "id": "pIL7lDzfK_YU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f():\n",
        "  arrs = jax.live_arrays(\"tpu\")\n",
        "  for idx, arr in enumerate(arrs):\n",
        "    print(idx, deetnosum(arr))\n",
        "\n",
        "f()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHeOpX6IKhjE",
        "outputId": "07cbba25-f019-424e-acdb-dca39eeec597"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 ((1024, 64), dtype('complex64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "1 ((1024, 64), dtype('complex64'), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "2 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "3 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "4 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "5 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "6 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "7 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "8 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "9 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "10 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "11 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "12 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "13 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "14 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "15 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "16 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "17 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "18 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "19 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "20 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "21 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "22 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "23 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "24 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "25 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "26 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "27 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "28 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "29 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "30 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "31 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "32 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "33 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "34 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "35 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "36 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "37 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "38 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "39 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "40 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "41 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "42 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "43 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "44 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "45 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "46 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "47 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "48 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "49 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "50 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "51 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "52 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "53 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "54 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "55 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "56 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "57 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "58 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "59 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "60 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "61 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "62 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "63 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "64 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "65 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "66 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "67 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "68 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "69 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "70 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "71 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "72 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "73 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "74 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "75 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "76 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "77 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "78 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "79 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "80 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "81 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "82 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "83 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "84 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "85 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "86 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "87 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "88 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "89 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "90 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "91 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "92 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "93 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "94 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "95 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "96 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "97 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "98 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "99 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "100 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "101 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "102 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "103 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "104 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "105 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "106 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "107 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "108 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "109 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "110 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "111 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "112 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "113 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "114 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "115 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "116 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "117 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "118 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "119 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "120 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "121 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "122 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "123 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "124 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "125 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "126 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "127 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "128 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "129 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "130 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "131 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "132 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "133 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "134 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "135 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "136 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "137 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "138 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "139 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "140 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "141 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "142 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "143 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "144 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "145 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "146 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "147 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "148 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "149 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "150 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "151 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "152 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "153 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "154 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "155 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "156 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "157 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "158 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "159 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "160 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "161 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "162 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "163 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "164 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "165 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "166 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "167 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "168 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "169 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "170 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "171 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "172 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "173 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "174 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "175 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "176 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "177 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "178 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "179 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "180 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "181 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "182 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "183 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "184 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "185 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "186 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "187 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "188 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "189 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "190 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "191 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "192 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "193 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "194 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "195 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "196 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "197 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "198 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "199 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "200 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "201 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "202 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "203 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "204 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "205 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "206 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "207 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "208 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "209 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "210 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "211 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "212 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "213 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "214 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "215 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "216 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "217 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "218 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "219 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "220 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "221 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "222 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "223 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "224 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "225 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "226 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "227 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "228 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "229 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "230 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "231 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "232 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "233 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "234 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "235 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "236 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "237 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "238 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "239 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "240 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "241 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "242 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "243 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "244 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "245 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "246 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "247 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "248 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "249 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "250 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "251 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "252 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "253 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "254 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "255 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "256 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "257 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "258 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "259 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "260 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "261 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "262 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "263 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "264 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "265 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "266 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "267 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "268 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "269 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "270 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "271 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "272 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "273 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "274 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "275 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "276 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "277 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "278 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "279 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "280 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "281 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "282 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "283 ((14336, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "284 ((4096, 14336), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "285 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "286 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "287 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "288 ((4096, 1024), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "289 ((4096, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "290 ((4096,), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "291 ((4096, 128256), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n",
            "292 ((128256, 4096), dtype(bfloat16), {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)})\n"
          ]
        }
      ]
    }
  ]
}